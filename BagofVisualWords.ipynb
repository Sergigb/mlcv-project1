{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2_Assignment\n",
    "## Team members:\n",
    "- Marc PÃ©rez Quintana  <br>\n",
    "- Basem Elbarashy <br>\n",
    "- Sergi Garcia Bordils <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- We list/explain at the start of each section the defined variables that will be used in the other sections \n",
    "- We assume that you have the following in the current dir:  \n",
    "test/, train/ , test_images_filenames.dat , train_images_filenames.dat, test_labels.dat, train_labels.dat\n",
    "- The code is tested with python 3 and opencv 3.4\n",
    "- Most components are implemented in methods so we can easily play with them at the end of the notebook and tune the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle as cPickle\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400 #number of features for the SIFT detector but for densesift it depends on step value\n",
    "step = 10\n",
    "k = 500 # codebook size / number of clusters for KMeans / number of words\n",
    "num_neighbors = 5 #number of neighbors (k) for the k-nn classifier\n",
    "knn_metric = 'manhattan'#distance for the k-nn classifier\n",
    "denseSift = True #True if Dense SIFT is to be used, False for classical SIFT\n",
    "pyramidDepth = 2 # 0-> No spatial pyramid, 1-> whole image + 4 subimages, 2-> lower levels + 16 subimages, ...\n",
    "\n",
    "\n",
    "normalization = True \n",
    "norm = 'power'  # l2 or power\n",
    "steps = [10]  # [10, 10, 10, 10]  # steps for the different desc sizes\n",
    "kpt_sizes = [10]  # [5, 10, 15, 20]  # desc sizes\n",
    "alpha = 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read the train and test files\n",
    "- train_images_filenames\n",
    "- test_images_filenames\n",
    "- train_labels\n",
    "- test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Trainset Size =  1881\n",
      "- Trainset classes count :  {'coast': 244, 'Opencountry': 292, 'street': 212, 'tallbuilding': 248, 'forest': 227, 'mountain': 260, 'highway': 184, 'inside_city': 214}\n",
      "- Testset Size =  807\n",
      "- Testset classes count:  {'coast': 116, 'Opencountry': 118, 'street': 80, 'tallbuilding': 108, 'forest': 101, 'mountain': 114, 'highway': 76, 'inside_city': 94}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images_filenames = cPickle.load(open('train_images_filenames.dat','rb'))\n",
    "test_images_filenames = cPickle.load(open('test_images_filenames.dat','rb'))\n",
    "train_labels = cPickle.load(open('train_labels.dat','rb'))\n",
    "test_labels = cPickle.load(open('test_labels.dat','rb'))\n",
    "\n",
    "class_count_test = {}\n",
    "for label in test_labels:\n",
    "    if label in class_count_test:\n",
    "        class_count_test[label] += 1\n",
    "    else: \n",
    "        class_count_test[label] = 1\n",
    "        \n",
    "class_count_train = {}\n",
    "for label in train_labels:\n",
    "    if label in class_count_train:\n",
    "        class_count_train[label] += 1\n",
    "    else: \n",
    "        class_count_train[label] = 1\n",
    "\n",
    "print('- Trainset Size = ', len(train_labels))\n",
    "print(\"- Trainset classes count : \", class_count_train)\n",
    "print('- Testset Size = ', len(test_labels))\n",
    "print(\"- Testset classes count: \", class_count_test)\n",
    "\n",
    "plt.bar(range(len(class_count_train)), list(class_count_train.values()), align='center')\n",
    "plt.xticks(range(len(class_count_train)), list(class_count_train.keys()), rotation='vertical')\n",
    "plt.title('Classes Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\\nskf_split = skf.split(train_images_filenames, train_labels)\\n\\nfor train_index, validation_index in skf_split:\\n    print(\"length of validation_index\", len(validation_index))\\n    print(\"VALIDATION:\", validation_index)\\n    class_count = {}\\n    for index in validation_index:\\n        label = train_labels[index]\\n        if label in class_count:\\n            class_count[label] += 1\\n        else: \\n            class_count[label] = 1\\n    print(class_count)\\n    print(\"-------------------------\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "skf_split = skf.split(train_images_filenames, train_labels)\n",
    "\n",
    "for train_index, validation_index in skf_split:\n",
    "    print(\"length of validation_index\", len(validation_index))\n",
    "    print(\"VALIDATION:\", validation_index)\n",
    "    class_count = {}\n",
    "    for index in validation_index:\n",
    "        label = train_labels[index]\n",
    "        if label in class_count:\n",
    "            class_count[label] += 1\n",
    "        else: \n",
    "            class_count[label] = 1\n",
    "    print(class_count)\n",
    "    print(\"-------------------------\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Dense SIFT\n",
    "- compute the SIFT descriptors for all the train images and subsequently build a numpy array with all the descriptors stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseSIFT(images_filenames, labels):\n",
    "    SIFTdetector = cv2.xfeatures2d.SIFT_create(num_features) # Create a SIFT object detector and descriptor\n",
    "    descriptors = []\n",
    "    label_per_descriptor = []\n",
    "    kpts = []\n",
    "    \n",
    "    if denseSift:\n",
    "        kpt = []\n",
    "        # I moved this here to avoid computing the kpts for every image, since they all have the same size\n",
    "        for step, size in zip(steps, kpt_sizes):  \n",
    "            kpt.extend([cv2.KeyPoint(x, y, size) for y in range(0, 256, step) \n",
    "                                             for x in range(0, 256, step)])\n",
    "    \n",
    "    for filename, labels in zip(images_filenames, labels):\n",
    "        filename = filename.replace(\"../../Databases/MIT_split\", \".\")\n",
    "        ima = cv2.imread(filename)\n",
    "        gray = cv2.cvtColor(ima, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if denseSift:\n",
    "            _, des = SIFTdetector.compute(gray, kpt)\n",
    "\n",
    "        else:\n",
    "            kpt, des = SIFTdetector.detectAndCompute(gray, None)\n",
    "        \n",
    "        if normalization:\n",
    "            if norm == 'l2':\n",
    "                des = normalize(des, norm, axis=0)\n",
    "            if norm == 'power':\n",
    "                des = np.sign(des) * (des**alpha)\n",
    "                \n",
    "        kpts.append(kpt)\n",
    "        descriptors.append(des)\n",
    "        label_per_descriptor.append(labels)\n",
    "\n",
    "    return (kpts, descriptors, np.vstack(descriptors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each train image, we project each keypoint descriptor to its closest visual word. We represent each of the images with the frequency of each visual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_words(codebook, descriptors, kpts):\n",
    "    num_images = len(descriptors)\n",
    "    for level in range(pyramidDepth+1):\n",
    "        if(level == 0):\n",
    "            pyramid_visual_words = np.zeros((num_images, k), dtype=np.float32)\n",
    "            for image in range(num_images):\n",
    "                for word in codebook.predict(descriptors[image]):   \n",
    "                    pyramid_visual_words[image,word]+=1\n",
    "                pyramid_visual_words[image] = pyramid_visual_words[image] / pyramid_visual_words[image].sum()\n",
    "            print(\"level 0 ->>>>>> \"+str(pyramid_visual_words.sum()))\n",
    "        else:\n",
    "            for x in range(2**level):\n",
    "                    for y in range(2**level): \n",
    "                        visual_words=np.zeros((num_images,k),dtype=np.float32)\n",
    "                        for image in range(num_images):    \n",
    "                            words = codebook.predict(descriptors[image])\n",
    "                            for keypoint in range(len(kpts[image])):\n",
    "                                x_pt, y_pt = kpts[image][keypoint].pt\n",
    "                                if (x_pt>=x*256/(2**level) and x_pt<(x+1)*256/(2**level) and y_pt>=y*256/(2**level) and y_pt<(y+1)*256/(2**level)):\n",
    "                                    visual_words[image, words[keypoint]]+=1\n",
    "                            visual_words[image] = visual_words[image] / visual_words[image].sum()\n",
    "                        pyramid_visual_words = np.append(pyramid_visual_words, visual_words, axis=1)\n",
    "                        print(\"level \"+str(level)+\",x \"+str(x)+\",y \"+str(y)+\": \"+str(visual_words.sum()))\n",
    "                        print(x*256/(2**level), (x+1)*256/(2**level), y*256/(2**level), (y+1)*256/(2**level))\n",
    "    return pyramid_visual_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramIntersection(M,N):\n",
    "    kernel = np.zeros((M.shape[0], N.shape[0]))\n",
    "    for d in range(M.shape[1]):\n",
    "        column_1 = M[:, d].reshape(-1, 1)\n",
    "        column_2 = N[:, d].reshape(-1, 1)\n",
    "        kernel += np.minimum(column_1, column_2.T)\n",
    "        #print(np.minimum(column_1, column_2.T).shape)\n",
    "        #for i in range(column_1.shape[0]):\n",
    "        #    for j in range(column_2.shape[0]):\n",
    "        #        kernel[i,j] = np.minimum(column_1[i],column_2[j])\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### Raw data ==> Densesift descriptors ==> Visual words ==> Visual words pyramid ==> Feature standarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0 ->>>>>> 1880.9999\n",
      "level 1,x 0,y 0: 1880.9999\n",
      "0.0 128.0 0.0 128.0\n",
      "level 1,x 0,y 1: 1880.9998\n",
      "0.0 128.0 128.0 256.0\n",
      "level 1,x 1,y 0: 1881.0001\n",
      "128.0 256.0 0.0 128.0\n",
      "level 1,x 1,y 1: 1881.0\n",
      "128.0 256.0 128.0 256.0\n",
      "level 2,x 0,y 0: 1880.9998\n",
      "0.0 64.0 0.0 64.0\n",
      "level 2,x 0,y 1: 1881.0005\n",
      "0.0 64.0 64.0 128.0\n",
      "level 2,x 0,y 2: 1880.9999\n",
      "0.0 64.0 128.0 192.0\n",
      "level 2,x 0,y 3: 1881.0004\n",
      "0.0 64.0 192.0 256.0\n",
      "level 2,x 1,y 0: 1880.9999\n",
      "64.0 128.0 0.0 64.0\n",
      "level 2,x 1,y 1: 1881.0002\n",
      "64.0 128.0 64.0 128.0\n",
      "level 2,x 1,y 2: 1881.0004\n",
      "64.0 128.0 128.0 192.0\n",
      "level 2,x 1,y 3: 1881.0005\n",
      "64.0 128.0 192.0 256.0\n",
      "level 2,x 2,y 0: 1881.0001\n",
      "128.0 192.0 0.0 64.0\n",
      "level 2,x 2,y 1: 1881.0004\n",
      "128.0 192.0 64.0 128.0\n",
      "level 2,x 2,y 2: 1880.9998\n",
      "128.0 192.0 128.0 192.0\n",
      "level 2,x 2,y 3: 1880.9995\n",
      "128.0 192.0 192.0 256.0\n",
      "level 2,x 3,y 0: 1881.0005\n",
      "192.0 256.0 0.0 64.0\n",
      "level 2,x 3,y 1: 1880.9998\n",
      "192.0 256.0 64.0 128.0\n",
      "level 2,x 3,y 2: 1880.9999\n",
      "192.0 256.0 128.0 192.0\n",
      "level 2,x 3,y 3: 1881.0001\n",
      "192.0 256.0 192.0 256.0\n",
      "level 0 ->>>>>> 806.99994\n",
      "level 1,x 0,y 0: 806.99994\n",
      "0.0 128.0 0.0 128.0\n",
      "level 1,x 0,y 1: 807.0\n",
      "0.0 128.0 128.0 256.0\n",
      "level 1,x 1,y 0: 807.0\n",
      "128.0 256.0 0.0 128.0\n",
      "level 1,x 1,y 1: 807.00006\n",
      "128.0 256.0 128.0 256.0\n",
      "level 2,x 0,y 0: 807.00006\n",
      "0.0 64.0 0.0 64.0\n",
      "level 2,x 0,y 1: 807.00006\n",
      "0.0 64.0 64.0 128.0\n",
      "level 2,x 0,y 2: 807.00006\n",
      "0.0 64.0 128.0 192.0\n",
      "level 2,x 0,y 3: 806.99994\n",
      "0.0 64.0 192.0 256.0\n",
      "level 2,x 1,y 0: 806.99994\n",
      "64.0 128.0 0.0 64.0\n",
      "level 2,x 1,y 1: 807.0\n",
      "64.0 128.0 64.0 128.0\n",
      "level 2,x 1,y 2: 807.0001\n",
      "64.0 128.0 128.0 192.0\n",
      "level 2,x 1,y 3: 807.00006\n",
      "64.0 128.0 192.0 256.0\n",
      "level 2,x 2,y 0: 806.99994\n",
      "128.0 192.0 0.0 64.0\n",
      "level 2,x 2,y 1: 806.99976\n",
      "128.0 192.0 64.0 128.0\n",
      "level 2,x 2,y 2: 806.99994\n",
      "128.0 192.0 128.0 192.0\n",
      "level 2,x 2,y 3: 807.00006\n",
      "128.0 192.0 192.0 256.0\n",
      "level 2,x 3,y 0: 807.00006\n",
      "192.0 256.0 0.0 64.0\n",
      "level 2,x 3,y 1: 806.99994\n",
      "192.0 256.0 64.0 128.0\n",
      "level 2,x 3,y 2: 806.99994\n",
      "192.0 256.0 128.0 192.0\n",
      "level 2,x 3,y 3: 806.99976\n",
      "192.0 256.0 192.0 256.0\n",
      "traindata size:  1881\n",
      "Feature size:  10500\n"
     ]
    }
   ],
   "source": [
    "# Densesift descriptors\n",
    "Train_kpts, Train_descriptors, Train_descriptors_vstacked = denseSIFT(train_images_filenames, train_labels)\n",
    "Test_kpts, Test_descriptors, Test_descriptors_vstacked    = denseSIFT(test_images_filenames,   test_labels)\n",
    "\n",
    "# Visual words\n",
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "codebook.fit(Train_descriptors_vstacked)\n",
    "\n",
    "#  Visual words pyramid\n",
    "visual_words_train = get_visual_words(codebook, Train_descriptors, Train_kpts)\n",
    "visual_words_test  = get_visual_words(codebook, Test_descriptors, Test_kpts)\n",
    "\n",
    "# Feature standarization\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(visual_words_train)\n",
    "\n",
    "visual_words_train = scaler.transform(visual_words_train)\n",
    "visual_words_test  = scaler.transform(visual_words_test)\n",
    "\n",
    "# Compare between data size and feature size\n",
    "print('traindata size: ',visual_words_train.shape[0])\n",
    "print('Feature size: ',visual_words_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.34696406443618\n"
     ]
    }
   ],
   "source": [
    "## Knn classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=num_neighbors,n_jobs=-1,metric=knn_metric)\n",
    "knn.fit(visual_words_train, train_labels)\n",
    "accuracy = 100*knn.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.98017348203221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n## SVM classifier with SGD!\\nclf = SGDClassifier(alpha=.0001, n_iter=60, penalty='l2', shuffle=True, random_state=0,verbose=0)\\nclf.fit(visual_words, train_labels)\\naccuracy = 100*clf.score(visual_words_test, test_labels)\\nprint(accuracy)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SVM classifier\n",
    "svm_kernel = histogramIntersection  # It must be one of âlinearâ, âpolyâ, ârbfâ, âsigmoidâ, âprecomputedâ or a callable \n",
    "clf = svm.SVC(kernel=svm_kernel, C = 0.01)\n",
    "clf.fit(visual_words_train, train_labels)\n",
    "accuracy = 100*clf.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n",
    "\n",
    "\"\"\"\n",
    "## SVM classifier with SGD!\n",
    "clf = SGDClassifier(alpha=.0001, n_iter=60, penalty='l2', shuffle=True, random_state=0,verbose=0)\n",
    "clf.fit(visual_words, train_labels)\n",
    "accuracy = 100*clf.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5 Stratified Folds for Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    validation_split_num = 1\n",
    "    accuracys = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    skf_split = skf.split(train_images_filenames, train_labels)\n",
    "    skf_split = skf.split(train_images_filenames, train_labels)\n",
    "    for train_index, validation_index in skf_split:\n",
    "        print(validation_split_num)\n",
    "        cv_train_images_filenames = []\n",
    "        cv_train_labels = []\n",
    "        validation_images_filenames = []\n",
    "        validation_labels = []\n",
    "        for index in train_index:\n",
    "            cv_train_images_filenames.append(train_images_filenames[index])\n",
    "            cv_train_labels.append(train_labels[index])                \n",
    "        for index in validation_index:\n",
    "            validation_images_filenames.append(train_images_filenames[index])\n",
    "            validation_labels.append(train_labels[index]) \n",
    "\n",
    "        cv_Train_kpts, cv_Train_descriptors, D = denseSIFT(cv_train_images_filenames, cv_train_labels)\n",
    "        Validation_kpts, Validation_descriptors, D = denseSIFT(validation_images_filenames, validation_labels)\n",
    "        \n",
    "        codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "        codebook.fit(D)\n",
    "        \n",
    "        visual_words = get_visual_words(codebook, cv_Train_descriptors, cv_Train_kpts)\n",
    "        visual_words_validation = get_visual_words(codebook, Validation_descriptors, Validation_kpts)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=num_neighbors,n_jobs=-1,metric=knn_metric)\n",
    "        knn.fit(visual_words, cv_train_labels)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(visual_words)\n",
    "        visual_words = scaler.transform(visual_words)\n",
    "        visual_words_validation = scaler.transform(visual_words_validation)\n",
    "\n",
    "        accuracyKNN = 100*knn.score(visual_words_validation, validation_labels)\n",
    "        \n",
    "        ## SVM classifier\n",
    "        svm_kernel = 'rbf'  # It must be one of âlinearâ, âpolyâ, ârbfâ, âsigmoidâ, âprecomputedâ or a callable \n",
    "        clf = svm.SVC(kernel=svm_kernel, C = 1)\n",
    "        clf.fit(visual_words, cv_train_labels)\n",
    "        accuracySVM = 100*clf.score(visual_words_validation, validation_labels)\n",
    "        accuracys.append(accuracySVM)\n",
    "\n",
    "        with open('parameters_execution.log', 'a') as f:\n",
    "            f.write('denseSift: '+str(denseSift)+', '+\n",
    "                    'num_features: '+str(num_features)+', '+\n",
    "                    'k: '+str(k)+', '+\n",
    "                    'num_neighbors: '+str(num_neighbors)+', '+\n",
    "                    'knn_metric: '+str(knn_metric)+', '+\n",
    "                    'step: '+str(step)+', '+\n",
    "                    'pyramidDepth: '+str(pyramidDepth)+', '+\n",
    "                    'validation_split_num: '+str(validation_split_num)+', '+\n",
    "                    'normalization: '+str(normalization)+', '+\n",
    "                    'norm: '+str(norm)+', '+\n",
    "                    'alpha: '+str(alpha)+', '+\n",
    "                    'kpt_sizes: '+str(kpt_sizes)+', '+\n",
    "                    'accuracySVM: '+str(accuracySVM)+', '+\n",
    "                    'accuracyKNN: '+str(accuracyKNN)+'\\n')\n",
    "        validation_split_num += 1\n",
    "\n",
    "    return accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dict_k = {}\n",
    "for k in range(100,2001,100): #parameter to be optimized by cv and range \n",
    "    print(k)\n",
    "    acc = evaluate_model()\n",
    "    dict_k[k] = acc\n",
    "print(dict_k)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evaluate_model()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "- We found that the dataset is balanced and no need for using class_weight for loss claculation while training or any any other solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3kernel",
   "language": "python",
   "name": "m3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
