{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2_Assignment\n",
    "## Team members:\n",
    "- Marc PÃ©rez Quintana  <br>\n",
    "- Basem Elbarashy <br>\n",
    "- Sergi Garcia Bordils <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- We list/explain at the start of each section the defined variables that will be used in the other sections \n",
    "- We assume that you have the following in the current dir:  test/, train/ , test_images_filenames.dat , train_images_filenames.dat, test_labels.dat, train_labels.dat\n",
    "- The code is tested with python 3 and opencv 3.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle as cPickle\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read the train and test files\n",
    "- train_images_filenames\n",
    "- test_images_filenames\n",
    "- train_labels\n",
    "- test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Trainset Size =  1881\n",
      "- Trainset classes count :  {'Opencountry': 292, 'coast': 244, 'forest': 227, 'highway': 184, 'inside_city': 214, 'mountain': 260, 'street': 212, 'tallbuilding': 248}\n",
      "- Testset Size =  807\n",
      "- Testset classes count:  {'Opencountry': 118, 'coast': 116, 'forest': 101, 'highway': 76, 'inside_city': 94, 'mountain': 114, 'street': 80, 'tallbuilding': 108}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9aea71d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_images_filenames = cPickle.load(open('train_images_filenames.dat','rb'))\n",
    "test_images_filenames = cPickle.load(open('test_images_filenames.dat','rb'))\n",
    "train_labels = cPickle.load(open('train_labels.dat','rb'))\n",
    "test_labels = cPickle.load(open('test_labels.dat','rb'))\n",
    "\n",
    "class_count_test = {}\n",
    "for label in test_labels:\n",
    "    if label in class_count_test:\n",
    "        class_count_test[label] += 1\n",
    "    else: \n",
    "        class_count_test[label] = 1\n",
    "        \n",
    "class_count_train = {}\n",
    "for label in train_labels:\n",
    "    if label in class_count_train:\n",
    "        class_count_train[label] += 1\n",
    "    else: \n",
    "        class_count_train[label] = 1\n",
    "\n",
    "print('- Trainset Size = ', len(train_labels))\n",
    "print(\"- Trainset classes count : \", class_count_train)\n",
    "print('- Testset Size = ', len(test_labels))\n",
    "print(\"- Testset classes count: \", class_count_test)\n",
    "\n",
    "plt.bar(range(len(class_count_train)), list(class_count_train.values()), align='center')\n",
    "plt.xticks(range(len(class_count_train)), list(class_count_train.keys()), rotation='vertical')\n",
    "plt.title('Classes Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\\nskf_split = skf.split(train_images_filenames, train_labels)\\n\\nfor train_index, validation_index in skf_split:\\n    print(\"length of validation_index\", len(validation_index))\\n    print(\"VALIDATION:\", validation_index)\\n    class_count = {}\\n    for index in validation_index:\\n        label = train_labels[index]\\n        if label in class_count:\\n            class_count[label] += 1\\n        else: \\n            class_count[label] = 1\\n    print(class_count)\\n    print(\"-------------------------\")\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "skf_split = skf.split(train_images_filenames, train_labels)\n",
    "\n",
    "for train_index, validation_index in skf_split:\n",
    "    print(\"length of validation_index\", len(validation_index))\n",
    "    print(\"VALIDATION:\", validation_index)\n",
    "    class_count = {}\n",
    "    for index in validation_index:\n",
    "        label = train_labels[index]\n",
    "        if label in class_count:\n",
    "            class_count[label] += 1\n",
    "        else: \n",
    "            class_count[label] = 1\n",
    "    print(class_count)\n",
    "    print(\"-------------------------\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters of the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400 #number of features for the SIFT detector but for densesift it depends on step value\n",
    "step = 10\n",
    "k = 300 # codebook size / number of clusters for KMeans / number of words\n",
    "num_neighbors = 5 #number of neighbors (k) for the k-nn classifier\n",
    "knn_metric = 'manhattan'#distance for the k-nn classifier\n",
    "denseSift = True #True if Dense SIFT is to be used, False for classical SIFT\n",
    "pyramidDepth = 2 # 0-> No spatial pyramid, 1-> whole image + 4 subimages, 2-> lower levels + 16 subimages, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Dense SIFT\n",
    "- compute the SIFT descriptors for all the train images and subsequently build a numpy array with all the descriptors stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseSIFT(images_filenames, labels):\n",
    "    SIFTdetector = cv2.xfeatures2d.SIFT_create(num_features) # Create a SIFT object detector and descriptor\n",
    "    descriptors = []\n",
    "    label_per_descriptor = []\n",
    "    kpts = []\n",
    "\n",
    "    for filename, labels in zip(images_filenames, labels):\n",
    "        filename = filename.replace(\"../../Databases/MIT_split\", \".\")\n",
    "        ima = cv2.imread(filename)\n",
    "        gray = cv2.cvtColor(ima, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if denseSift:\n",
    "            height, width = gray.shape\n",
    "            kpt = [cv2.KeyPoint(x, y, step) for y in range(0, gray.shape[0], step) \n",
    "                                            for x in range(0, gray.shape[1], step)]\n",
    "            _, des = SIFTdetector.compute(gray, kpt)\n",
    "\n",
    "        else:\n",
    "            kpt, des = SIFTdetector.detectAndCompute(gray, None)\n",
    "\n",
    "        kpts.append(kpt)\n",
    "        descriptors.append(des)\n",
    "        label_per_descriptor.append(labels)\n",
    "\n",
    "    return (kpts, descriptors, np.vstack(descriptors))\n",
    "\n",
    "Train_kpts, Train_descriptors, D = denseSIFT(train_images_filenames, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute k-means clustering on the descriptor space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=6000, compute_labels=False, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=300, n_init=3, random_state=42,\n",
       "        reassignment_ratio=0.0001, tol=0.0, verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "codebook.fit(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each train image, we project each keypoint descriptor to its closest visual word. We represent each of the images with the frequency of each visual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_words(descriptors, kpts):\n",
    "    for level in range(pyramidDepth+1):\n",
    "        if(level == 0):\n",
    "            pyramid_visual_words = np.zeros((len(descriptors), k), dtype=np.float32)\n",
    "            for i in range(len(descriptors)):\n",
    "                for word in codebook.predict(descriptors[i]):   \n",
    "                    pyramid_visual_words[i,word]+=1\n",
    "        else:\n",
    "            for x in range(2**level):\n",
    "                    for y in range(2**level): \n",
    "                        visual_words=np.zeros((len(descriptors),k),dtype=np.float32)\n",
    "                        for i in range(len(descriptors)):    \n",
    "                            words = codebook.predict(descriptors[i])\n",
    "                            for keypoint in range(len(descriptors[i])):\n",
    "                                x_pt, y_pt = kpts[i][keypoint].pt\n",
    "                                if (x_pt>=x*256/(2**level) and x_pt<(x+1)*256/(2**level) and y_pt>=x*256/(2**level) and y_pt<(x+1)*256/(2**level)):\n",
    "                                    visual_words[i, words[keypoint]]+=1\n",
    "                        pyramid_visual_words = np.append(pyramid_visual_words, visual_words, axis=1)\n",
    "        #print(pyramid_visual_words.shape)\n",
    "    return pyramid_visual_words\n",
    "visual_words = get_visual_words(Train_descriptors, Train_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a k-nn classifier and train it with the train descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=num_neighbors,n_jobs=-1,metric=knn_metric)\n",
    "knn.fit(visual_words, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the test descriptors and compute the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_kpts, Test_descriptors, D = denseSIFT(test_images_filenames, test_labels)\n",
    "visual_words_test = get_visual_words(Test_descriptors, Test_kpts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.56257744733581\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100*knn.score(visual_words_test, test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1881, 6300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.19950433705081\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', C = 1.5, gamma='auto')\n",
    "clf.fit(visual_words, train_labels)\n",
    "accuracy = 100*clf.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.40396530359355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(alpha=.0001, n_iter=60, penalty='l2', shuffle=True, random_state=0,verbose=0)\n",
    "clf.fit(visual_words, train_labels)\n",
    "accuracy = 100*clf.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 5 Stratified Folds for Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    validation_split_num = 1\n",
    "    accuracys = []\n",
    "    skf_split = skf.split(train_images_filenames, train_labels)\n",
    "    for train_index, validation_index in skf_split:\n",
    "        print(validation_split_num)\n",
    "        cv_train_images_filenames = []\n",
    "        cv_train_labels = []\n",
    "        validation_images_filenames = []\n",
    "        validation_labels = []\n",
    "        for index in train_index:\n",
    "            cv_train_images_filenames.append(train_images_filenames[index])\n",
    "            cv_train_labels.append(train_labels[index])                \n",
    "        for index in validation_index:\n",
    "            validation_images_filenames.append(train_images_filenames[index])\n",
    "            validation_labels.append(train_labels[index]) \n",
    "\n",
    "        cv_Train_kpts, cv_Train_descriptors, D = denseSIFT(cv_train_images_filenames, cv_train_labels)\n",
    "        codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "        codebook.fit(D)\n",
    "        visual_words = get_visual_words(cv_Train_descriptors, cv_Train_kpts)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=num_neighbors,n_jobs=-1,metric=knn_metric)\n",
    "        knn.fit(visual_words, cv_train_labels)\n",
    "        \n",
    "        \n",
    "        Validation_kpts, Validation_descriptors, D = denseSIFT(validation_images_filenames, validation_labels)\n",
    "        visual_words_validation = get_visual_words(Validation_descriptors, Validation_kpts)\n",
    "        \n",
    "        accuracy = 100*knn.score(visual_words_validation, validation_labels)\n",
    "        accuracys.append(accuracy)\n",
    "\n",
    "        with open('parameters_execution.log', 'a') as f:\n",
    "            f.write('denseSift: '+str(denseSift)+', '+\n",
    "                    'num_features: '+str(num_features)+', '+\n",
    "                    'k: '+str(k)+', '+\n",
    "                    'num_neighbors: '+str(num_neighbors)+', '+\n",
    "                    'knn_metric: '+str(knn_metric)+', '+\n",
    "                    'step: '+str(step)+', '+\n",
    "                    'pyramidDepth: '+str(pyramidDepth)+', '+\n",
    "                    'validation_split_num: '+str(validation_split_num)+', '+\n",
    "                    'accuracy: '+str(accuracy)+'\\n')\n",
    "        validation_split_num += 1\n",
    "    return accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndict_k = {}\\nfor k in range(100,2001,100): #parameter to be optimized by cv and range \\n    print(k)\\n    acc = evaluate_model()\\n    dict_k[k] = acc\\nprint(dict_k)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dict_k = {}\n",
    "for k in range(100,2001,100): #parameter to be optimized by cv and range \n",
    "    print(k)\n",
    "    acc = evaluate_model()\n",
    "    dict_k[k] = acc\n",
    "print(dict_k)\n",
    "\"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We tuned parameters individually first and then tuned all at the same time\n",
    "### Visualizing how each parameter affect the accuracy after trying several values for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "k_range             = np.arange(20, 600, 20)\n",
    "num_neighbors_range = np.arange(1, 18, 1)\n",
    "step_range          = np.arange(10,40,5)\n",
    "knn_metric_range    = ['Chebyshev','Euclidean','Manhattan']\n",
    "\n",
    "k_acc               = [66.5,73.2,70.7,75.9,73.3,76.2,76.3,73.4,74.5,75.2,75.2,74.9,76.3,73.9,75.2,75.9,76.7,77.5,74.2,73.6,74.5,76.7,76.4,74.2,72.9,74.5,76.0, 75.9, 72.7]\n",
    "num_neighbors_acc   = [70.6,71.1,73.6,76.2,77.5,75.9,75.8,75.8,75.8,76.8,77.3,77.6,77.4,77.6,76.7,75.9, 76.5]\n",
    "step_acc            = [77.5, 74.3, 75.9, 75.7, 74.4, 73.3]\n",
    "knn_metric_acc      = [59.60,77.57, 78.31]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(221)\n",
    "plt.plot(k_range, k_acc, 'r--');  plt.plot(k_range, k_acc, 'o');  \n",
    "plt.xlabel('K');  plt.ylabel('Accuracy');\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(num_neighbors_range, num_neighbors_acc, 'r--');  plt.plot(num_neighbors_range, num_neighbors_acc, 'o');  \n",
    "plt.xlabel('Num_neighbors');  plt.ylabel('Accuracy');\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(221)\n",
    "plt.plot(step_range, step_acc, 'r--');  plt.plot(step_range, step_acc, 'o');  \n",
    "plt.xlabel('Step');  plt.ylabel('Accuracy');\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(knn_metric_range, knn_metric_acc, 'r--');  plt.plot(knn_metric_range, knn_metric_acc, 'o');  \n",
    "plt.xlabel('Knn_metric');  plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning all parameters at the same time, visualizing the accuracy versus different configurations of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "| denseSift | num_features   | k | num_neighbors | knn_metric | step | accuracy |\n",
    "|------|------|------|------|------|------|------|\n",
    "|   True  | 676 | 100  | 5 | manhattan  | 10 | 77.3  | \n",
    "|   True  | 676 | 100  | 5 | euclidean  | 10 | 73.3  | \n",
    "|   True  | 169 | 100  | 5 | manhattan  | 20 | 74.3  | \n",
    "|   True  | 169 | 100  | 5 | euclidean  | 20 | 74.3  | \n",
    "|   True  | 676 | 100  | 10 | manhattan  | 10 | 77.6  | \n",
    "|   True  | 676 | 100  | 10 | euclidean  | 10 | 74.9  | \n",
    "|   True  | 169 | 100  | 10 | manhattan  | 20 | 75.4  | \n",
    "|   True  | 169 | 100  | 10 | euclidean  | 20 | 74.5  | \n",
    "|   True  | 676 | 300  | 5 | manhattan  | 10 | 80.9  | \n",
    "|   True  | 676 | 300  | 5 | euclidean  | 10 | 75.2  | \n",
    "|   True  | 169 | 300  | 5 | manhattan  | 20 | 77.1  | \n",
    "|   True  | 169 | 300  | 5 | euclidean  | 20 | 75.9  | \n",
    "|   True  | 676 | 300  | 10 | manhattan  | 10 | 80.5  | \n",
    "|   True  | 676 | 300  | 10 | euclidean  | 10 | 75.3  | \n",
    "|   True  | 169 | 300  | 10 | manhattan  | 20 | 77.1  | \n",
    "|   True  | 169 | 300  | 10 | euclidean  | 20 | 76.4  | \n",
    "|   True  | 2704 | 300  | 10 | manhattan  | 5 | 80.7  | \n",
    "|   False  | 500 | 300  | 5 | manhattan  | - | 48.3  | \n",
    "|   False  | 800 | 300  | 5 | manhattan  | - | 52  | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "- We found that the dataset is balanced and no need for using class_weight for loss claculation while training or any any other solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python3.6",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
